The latest version of *Microsoft Macro Assembler (known as MASM)* should be used for these notes. MASM is included with Microsoft *Visual Studio*.

*Assembly language* is the oldest programming language, and of all languages, bears the closest resemblance to native machine language. It provides direct access to computer hardware, requiring you to understand much about your computer's architecture and operation system.

# 1.1 Welcome to Assembly Language

## What are Assemblers and Linkers?

An ***assembler*** is a utility program that converts source code programs from assembly language into machine language. A ***linker*** is a utility program that combines individual files created by an assembler into a single executable program.

## What Hardware and Software Do I Need?

You need a computer that runs a 32-bit or 64-bit version of Microsoft Windows, along with one of the recent versions of Microsoft Visual Studio.

## What Types of Programs Can Be Created Using MASM?

- ***32-Bit Protected Mode***: these programs run under all 32-bit and 64-bit version of Microsoft Windows. They are usually easier to write and understand than real-mode programs. We will simply call this *32-bit mode*.
- ***64-Bit Mode***: these programs run under all 64-bit versions of Microsoft Windows.

## What Will I Learn?

- Basic principles of computer architecture as applied to x86 processors
- Basic Boolean logic and how it applies to programming and computer hardware
- How x86 processors manage memory, using protected mode and virtual mode
- How high-level languages compilers, such as C++, translate statements from their language into assembly language and native machine code
- How high-level languages implement arithmetic expressions, loops, and logical structures at the machine level
- Data representation, including signed and unsigned integers, real numbers, and character data
- How to debug programs at the machine level. The need for this skill is vital when you work in languages such as C and C++ which generate native machine code
- How application programs communicate with the computer's operating system via interrupt handlers and systems calls
- How to interface assembly language code to C++ programs
- How to create assembly language application programs

## How Does Assembly Language Relate to Machine Language?

***Machine Languages*** is a numeric language specifically understood by a computer's processor. All x86 processors understand a common machine language. *Assembly language* consists of statements written with short mnemonics such ADD, MOV, SUB, and CALL. Assembly language has a *one-to-one* relationship with machine language: Each assembly language instruction corresponds to a single machine language instruction.

## Is Assembly Language Portable?

Assembly language is not portable, because it is designed for a specific processor family. There are a number of different assembly languages widely used today, each based on a processor family. Some well-known processor families are Motorola 68x00, SUN Sparc, Vax, and IBM-370. The instruction in assembly language may directly match the computer's architecture or they may be translated during execution by a program inside the processor known as the ***microcode interpreter***.

## Why Learn Assembly Language?

- Real-time applications dealing with simulation and hardware monitoring require precise timing and responses. High-level languages do not give programmers exact control over machine code generated by compilers. Assembly language permits you to precisely specify a program's executable code. 
- Assembly language helps you gain an overall understanding of the interaction between computer hardware, operating systems, and application programs. Using assembly, you can apply and test theoretical information you are given in computer architecture and operating systems courses.
- Some high-level languages abstract their data representation to the point that it becomes awkward to perform low-level tasks such as bit manipulation. In such an environment, programmers will often call subroutines written in assembly language to accomplish their goal.

# 1.2 Virtual Machine Concept

A computer can usually execute programs written in its native *machine language*. Each instruction in this language is simple enough to be executed using a relatively small number of electronic circuits. For simplicity, we will call this language **L0**.

Programmers would have a difficult time writing programs in L0 because it is enormously detailed and consists of purely numbers. If a new language **L1**, could be constructed that was easier to use, programs could be written in L1.

- *Interpretation*: As the L1 program is running, each of its instructions could be decoded and executed by a program written in language L0. The L1 program begins running immediately, but each instruction has to be decoded before it can execute, often causing a small time delay.
- *Translation*: The entire L1 program could be converted into an L0 program by an L0 program specifically designed for this purpose. The resulting L0 program could be executed directly on the computer hardware.

## Virtual Machines

Informally, we can define a ***virtual machine (VM)*** as a software program that emulates the functions of some other physical or virtual computer. The virtual machine **VM1**, can execute commands written in language L1. The virtual machine **VM0** can execute commands written in language L0:

- Virtual Machine VM1
- Virtual Machine VM0

> The Java programming language is based on the virtual machine concept. A program written in the Java Programming language is translated by a Java compiler into *Java byte code*. The latter is a low-level language quickly executed at runtime by a program known as a *Java virtual machine (JVM)*. The JVM has been implemented on many different computer systems, making Java programs relatively system independent.

## Specific Machines

| Virtual Machine Levels | Language Level |
| :-: | :-: |
| Level 4 | High-level language |
| Level 3 | Assembly language |
| Level 2 | Instruction set architecture (ISA) |
| Level 1 | Digital Logic |

# 1.3 Data Representation

| System | Base | Possible Digits |
| :-: | :-: | :-: |
| Binary | 2 | 0 1 |
| Octal | 8 | 0 1 2 3 4 5 6 7 |
| Decimal | 10 | 0 1 2 3 4 5 6 7 8 9 |
| Hexadecimal | 16 | 0 1 2 3 4 5 6 7 8 9 A B C D E F |

***Binary numbers*** are base 2 numbers in which each binary digit, called a *bit* is either 0 or 1. **Bits** are numbered sequentially starting at zero on the right side and increasing toward the left. The bit on the left is the ***most significant bit (MSB)***, and the bit on the right is the ***least significant bit (LSB)***.

![[img1.png|500]]

| 2^n | Decimal Value | 2^n | Decimal Value |
| :-: | :-: | :-: | :-: |
| 2^0 | 1 | 2^8 | 256 |
| 2^1 | 2 | 2^9 | 512 |
| 2^2 | 4 | 2^10 | 1024 |
| 2^3 | 8 | 2^11 | 2048 |
| 2^4 | 16 | 2^12 | 4096 |
| 2^5 | 32 | 2^13 | 8192 |
| 2^6 | 64 | 2^14 | 16384 |
| 2^7 | 128 | 2^15 | 32768 |

## Translating Unsigned Decimal Integers to Binary

To translate an unsigned decimal integer into binary, repeatedly divide the integer by 2, saving each remainder as a binary digit. The following table shows the steps required to translate decimal 37 to binary. The resulting binary remainders are in *reverse order* for the binary number.

| Division | Quotient | Remainder |
| :-: | :-: | :-: |
| 37/2 | 18 | 1 |
| 18/2 | 9 | 0 |
| 9/2 | 4 | 1 |
| 4/2 | 2 | 0 |
| 2/2 | 1 | 0 |
| 1/2 | 0 | 1 |

Thus, according to the table, 37_d = 100101_b. Because computer storage always consists of binary numbers whose lengths are multiples of 8, we fill the remaining two digit positions on the left with zero, producing 00100101_b.

## Integer Storage Sizes

| Type | Range | Storage Size in Bits |
| :-- | :-- | :-: |
| Unsigned byte | 0 to 2^8-1 | 8 |
| Unsigned word | 0 to 2^16-1 | 16 |
| Unsigned doubleword | 0 to 2^32-1 | 32 |
| Unsigned quadword | 0 to 2^64-1 | 64 |
| Unsigned double quadword | 0 to 2^128-1 | 128 |

## Large Measurements

| Type | 2^n | Bytes |
| :-- | :-: | --: |
| kilobyte | 2^10 | 1,024 |
| megabyte | 2^20 | 1,048,576 |
| gigabyte | 2^30 | 1,073,741,824 |
| terabyte | 2^40 | 1,099,511,627,776 |
| petabyte | 2^50 | 1,125,899,906,842,624 |
| exabyte | 2^60 | 1,152,921,504,606,846,976 |
| zettabyte | 2^70 | 1,180,591,620,717,411,303,424 |
| yottabyte | 2^80 | 1,208,925,819,614,629,174,706,176 |

## Converting Unsigned Decimal to Hexadecimal

To convert an unsigned decimal integer to hexadecimal, repeatedly divide the decimal value by 16 and retain each remainder as a hexadecimal digit.

| Division | Quotient | Remainder |
| :-: | :-: | :-: |
| 422/16 | 26 | 6 |
| 26/16 | 1 | A |
| 1/16 | 0 | 1 |

Thus, according to the table, 422_d = 1A6_h.

> To convert from decimal into some other number base other than hexadecimal, replace the divisor in each calculation with the desired number base.

## Signed Binary Integers

***Signed binary integers*** are positive or negative. For x86 processors, the MSB indicates the sign; 0 is positive and 1 is negative.

![[img2.png|500]]

## Two's Complement Representation

Negative integers use ***two's-complement representation***, using the mathematical principle that the two's complement of an integer is its additive inverse. That is, if you add a number to its additive inverse, the sum is zero.

| Step | Example |
| :-- | :-: |
| 0. Starting value | 0000 0001 |
| 1. Reverse the bits | 1111 1110 |
| 2. Add 1 to value from Step 1 | 1111 1110 <br /> +0000 0001 |
| 3. Sum is two's complement representation | 1111 1111 |

## Maximum and Minimum Values

A signed integer of *n* bits uses only *n-1* bits to represent the number's magnitude.

| Type | Range | Storage Size in Bits |
| :-- | --: | :-: |
| Signed byte | -2^7 to +2^7-1 | 8 |
| Signed word | -2^15 to +2^15-1 | 16 |
| Signed doubleword | -2^31 to + 2^31-1 | 32 |
| Signed quadword | -2^63 to +2^63-1 | 64 |
| Signed double quadword | -2^127 to +2^127-1 | 128 |

## Binary Subtraction

A simple way to approach binary subtraction is to reverse the sign of the value begin subtracted, and then add the two values.

| Example | |
| --: | --- |
| 01101 | (+13) |
| 11001 | (-7) |
| = 00110 | (+6) |
## Character Storage

IBM-compatible microcomputers use the ***ASCII*** character set. ASCII is an acronym for *American Standard Code for Information Interchange*. In ASCII, a unique 7-bit integer is assigned to each character. Because ASCII codes only use the lower 7 bits of every byte, the extra bit is used on various microcomputers, for example, 128-255 represent graphic symbols and Greek characters.

## ANSI Character Set

The American National Standard Institute (ANSI) defines an 8-bit character set that represents up to 256 characters. The first 128 characters correspond to the letters and symbols on a standard U.S. keyboard. The second 128 characters represent special characters such as letters in international alphabets, accents, currency symbols, and fractions.

## Unicode Standard

The ***Unicode*** standard was created as a universal way of defining characters and symbols. It defines numeric code (called ***code points***) for characters, symbols, and punctuation used in all major languages, as well as European alphabetic scripts, Middle Eastern right-to-left scripts, and many scripts of Asia. Three transformation formats are used to transform code points into displayable characters called ***Unicode Transformation Format (UTF)***.
- **UTF-8** is used in HTML, and has the same byte values as ASCII.
- **UTF-16** is used in environments that balance efficient access to characters with economical use of storage. Each character is encoded in 16 bits.
- **UTF-32** is used in environments where place is no concern and fixed-width characters are required. Each character is encoded in 32 bits.
A ***null-terminated string*** is a string of characters followed by a single byte containing zero. The C and C++ languages use null-terminated strings, and many Windows operating systems functions requires string to be in this format.

## ASCII Control Characters

Character codes in the range 0 through 31 are called ***ASCII control characters***.

| ASCII Code (Decimal) | Description | |
| :-: | :-- | :-- |
| 8 | Backspace | (moves one column to the left) |
| 9 | Horizontal tab | (skips forward *n* columns) |
| 10 | Line feed | (moves to next output line) |
| 12 | Form feed | (moves to next printer page) |
| 13 | Carriage return | (moves to leftmost output column) |
| 27 | Escape character|

# 1.4 Boolean Expressions

*Boolean algebra* defines a set of operations on the value of **true** and **false**. A *Boolean expression* involves a Boolean operator and one or more operands. To see more on concepts in Boolean algebra, see [[discrete-math.pdf]]

To Continue
[[2. x86 Processor Architecture]]